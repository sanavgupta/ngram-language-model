{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58156a6",
   "metadata": {},
   "source": [
    "# Building Language Models from Scratch\n",
    "\n",
    "In this notebook, we explore the statistical foundations of Natural Language Processing. We will build three types of language models to estimate the probability of a sentence $P(w)$:\n",
    "\n",
    "1.  **Uniform Model**: All tokens are equally likely.\n",
    "2.  **Unigram Model**: Probabilities are based on word frequency.\n",
    "3.  **N-Gram Model**: Probabilities depend on the previous $N-1$ tokens (The Markov Assumption).\n",
    "\n",
    "We will conclude by training an N-Gram model on Shakespeare to generate synthetic Shakespearean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6727478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffde034",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition & Preprocessing\n",
    "\n",
    "To train a model, we first need data. The function below fetches books from [Project Gutenberg](https://www.gutenberg.org/) and strips out metadata headers/footers. We then `tokenize` the text, splitting it into words and punctuation while preserving paragraph structures using `\\x02` (Start) and `\\x03` (Stop) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1226af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(url):\n",
    "    \"\"\"\n",
    "    Fetches a book from Project Gutenberg and strips metadata.\n",
    "    \"\"\"\n",
    "    robots_url = \"https://www.gutenberg.org/robots.txt\"\n",
    "    try:\n",
    "        # Respect robots.txt (simple delay)\n",
    "        time.sleep(0.5)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    r = requests.get(url)\n",
    "    text = r.text.replace('\\r\\n', '\\n')\n",
    "    \n",
    "    # Regex to find the actual start and end of the book content\n",
    "    start_pattern = r\"^\\*+\\s*START\\s+OF\\s+(?:THE|THIS)\\s+PROJECT\\s+GUTENBERG\\s+EBOOK[^\\n]*\\*+$\"\n",
    "    end_pattern = r\"^\\*+\\s*END\\s+OF\\s+(?:THE|THIS)\\s+PROJECT\\s+GUTENBERG\\s+EBOOK[^\\n]*\\*+$\"\n",
    "    \n",
    "    start_match = re.search(start_pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "    end_match = re.search(end_pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    if not start_match or not end_match:\n",
    "        # Fallback if patterns aren't found (return full text)\n",
    "        return text\n",
    "        \n",
    "    start_idx = start_match.end()\n",
    "    end_idx = end_match.start()\n",
    "    return text[start_idx:end_idx]\n",
    "\n",
    "def tokenize(book_string):\n",
    "    \"\"\"\n",
    "    Tokenizes text into a list of tokens, adding START (\\x02) \n",
    "    and STOP (\\x03) tokens for paragraphs.\n",
    "    \"\"\"\n",
    "    book_string = book_string.strip()\n",
    "    if not book_string:\n",
    "        return ['\\x02', '\\x03']\n",
    "\n",
    "    paragraphs = re.split(r'\\n{2,}', book_string)\n",
    "    tokens = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        tokens.append('\\x02')\n",
    "        # Split by words or punctuation\n",
    "        para_tokens = re.findall(r'\\w+|[^\\w\\s]', para, re.UNICODE)\n",
    "        tokens.extend(para_tokens)\n",
    "        tokens.append('\\x03')\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d9e7e",
   "metadata": {},
   "source": [
    "## 2. Baseline Models\n",
    "\n",
    "Before building the N-Gram model, we establish two baselines:\n",
    "* **UniformLM**: $P(w) = \\frac{1}{|V|}$ where $V$ is the vocabulary size.\n",
    "* **UnigramLM**: $P(w) = \\frac{Count(w)}{N}$ where $N$ is total tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd518640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformLM:\n",
    "    def __init__(self, tokenized_corpus):\n",
    "        self.mdl = None\n",
    "        self.train(tokenized_corpus)\n",
    "\n",
    "    def train(self, tokens):\n",
    "        unique_tokens = pd.Series(list(set(tokens)))\n",
    "        prob = 1 / len(unique_tokens)\n",
    "        self.mdl = pd.Series(prob, index=unique_tokens)\n",
    "        return self.mdl\n",
    "\n",
    "    def probability(self, token_tuple):\n",
    "        if any(token not in self.mdl.index for token in token_tuple):\n",
    "            return 0\n",
    "        probs = self.mdl.loc[list(token_tuple)]\n",
    "        return np.prod(probs)\n",
    "\n",
    "    def sample(self, M):\n",
    "        sampled_tokens = np.random.choice(self.mdl.index, size=M, replace=True)\n",
    "        return ' '.join(sampled_tokens)\n",
    "\n",
    "class UnigramLM:\n",
    "    def __init__(self, tokens):\n",
    "        self.mdl = self.train(tokens)\n",
    "    \n",
    "    def train(self, tokens):\n",
    "        token_series = pd.Series(tokens)\n",
    "        counts = token_series.value_counts()\n",
    "        probs = counts / counts.sum()\n",
    "        return probs\n",
    "    \n",
    "    def probability(self, words):\n",
    "        if any(word not in self.mdl.index for words in words):\n",
    "            return 0\n",
    "        probs = self.mdl.loc[list(words)]\n",
    "        return np.prod(probs)\n",
    "        \n",
    "    def sample(self, M):\n",
    "        sampled_tokens = np.random.choice(self.mdl.index, size=M, replace=True, p=self.mdl.values)\n",
    "        return ' '.join(sampled_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1f5bd",
   "metadata": {},
   "source": [
    "## 3. The N-Gram Model\n",
    "\n",
    "The N-Gram model assumes that the probability of a token depends only on the previous $N-1$ tokens:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) \\approx P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "We estimate these probabilities using empirical counts from our training corpus:\n",
    "\n",
    "$$P(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n)}{C(w_{n-1})}$$\n",
    "\n",
    "The class below implements this logic using pandas DataFrames to store the conditional probabilities. It also includes a recursive structure (`prev_mdl`) to handle the start of sentences where fewer than $N-1$ tokens exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ad14333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM:\n",
    "    def __init__(self, N, tokens):\n",
    "        self.N = N\n",
    "        ngrams = self.create_ngrams(tokens)\n",
    "        self.ngrams = ngrams\n",
    "        self.mdl = self.train(ngrams)\n",
    "\n",
    "        if N < 2:\n",
    "            raise Exception('N must be greater than 1')\n",
    "        elif N == 2:\n",
    "            self.prev_mdl = UnigramLM(tokens)\n",
    "        else:\n",
    "            self.prev_mdl = NGramLM(N-1, tokens)\n",
    "\n",
    "    def create_ngrams(self, tokens):\n",
    "        \"\"\"Creates a list of sliding window N-grams.\"\"\"\n",
    "        return [tuple(tokens[i:i+self.N]) for i in range(len(tokens)-self.N+1)]\n",
    "        \n",
    "    def train(self, ngrams):\n",
    "        \"\"\"Calculates conditional probabilities for the N-grams.\"\"\"\n",
    "        counts = {}\n",
    "        n1_counts = {}\n",
    "    \n",
    "        for ng in ngrams:\n",
    "            counts[ng] = counts.get(ng, 0) + 1\n",
    "            n1 = ng[:-1]\n",
    "            n1_counts[n1] = n1_counts.get(n1, 0) + 1\n",
    "    \n",
    "        rows = []\n",
    "        for ng, c in counts.items():\n",
    "            n1 = ng[:-1]\n",
    "            prob = c / n1_counts[n1]\n",
    "            rows.append({'ngram': ng, 'n1gram': n1, 'prob': prob})\n",
    "    \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def probability(self, words):\n",
    "        \"\"\"Calculates the probability of a sequence of words.\"\"\"\n",
    "        words = tuple(words)\n",
    "        if len(words) == 0:\n",
    "            return 0.0\n",
    "    \n",
    "        prob = 1.0\n",
    "        for i in range(len(words)):\n",
    "            context_len = min(i, self.N - 1)\n",
    "            context = words[i - context_len:i]\n",
    "            target = words[i]\n",
    "            ngram = context + (target,)\n",
    "    \n",
    "            if len(ngram) < self.N:\n",
    "                model = self.prev_mdl\n",
    "                while hasattr(model, 'N') and model.N > len(ngram):\n",
    "                     model = model.prev_mdl\n",
    "                \n",
    "                if isinstance(model, UnigramLM):\n",
    "                     if target not in model.mdl.index: return 0.0\n",
    "                     prob *= float(model.mdl[target])\n",
    "                else:\n",
    "                    prob_df = model.mdl.set_index('ngram')['prob']\n",
    "                    if ngram not in prob_df.index: return 0.0\n",
    "                    prob *= float(prob_df[ngram])\n",
    "            else:\n",
    "                prob_df = self.mdl.set_index('ngram')['prob']\n",
    "                if ngram not in prob_df.index:\n",
    "                    return 0.0\n",
    "                prob *= float(prob_df[ngram])\n",
    "        return prob\n",
    "\n",
    "    def sample(self, M):\n",
    "        \"\"\"Generates a sentence of length M using backoff for the start.\"\"\"\n",
    "        output = ['\\x02']\n",
    "        \n",
    "        while len(output) < M + 1:\n",
    "            current_model = self\n",
    "            \n",
    "\n",
    "            while hasattr(current_model, 'N') and len(output) < current_model.N:\n",
    "                 current_model = current_model.prev_mdl\n",
    "            \n",
    "            if isinstance(current_model, UnigramLM):\n",
    "                next_token = np.random.choice(current_model.mdl.index, p=current_model.mdl.values)\n",
    "                output.append(next_token)\n",
    "                continue\n",
    "\n",
    "            context = tuple(output[-(current_model.N-1):])\n",
    "            candidates = current_model.mdl[current_model.mdl['n1gram'] == context]\n",
    "            \n",
    "            if candidates.empty:\n",
    "                output.append('\\x03')\n",
    "                break\n",
    "            \n",
    "            next_token = np.random.choice(\n",
    "                candidates['ngram'].apply(lambda x: x[-1]),\n",
    "                p=candidates['prob']\n",
    "            )\n",
    "            output.append(next_token)\n",
    "            \n",
    "        return ' '.join(output[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c3435",
   "metadata": {},
   "source": [
    "## 4. Model Demonstration: Shakespeare\n",
    "\n",
    "Let's test our model on the Complete Works of Shakespeare. We will verify that the **N-Gram model** produces significantly more coherent text than the **Unigram model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ae58f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpus...\n",
      "Tokenization complete. Total tokens: 1340412\n",
      "Training Unigram Model...\n",
      "Training Trigram (N=3) Model...\n",
      "\n",
      "--- Unigram Sample (Random Words) ---\n",
      "not Sad play see \u0002 \u0002 , PAULINA such Than , disasters Into ’ \u0003 \u0003 a A have of\n",
      "\n",
      "--- Trigram Sample (Context-Aware) ---\n",
      "“ Your meat doth burn my study , let him live . \u0003 \u0002 SPEED . [ _To Northumberland_ . ] O my liege , Whom I would do were as fleet , That if one be good , That thee may glorify the Lord protect him from me are\n"
     ]
    }
   ],
   "source": [
    "# 1. Fetch Data\n",
    "# We download the full text of Shakespeare from Project Gutenberg\n",
    "url = 'https://www.gutenberg.org/ebooks/100.txt.utf-8' \n",
    "print(\"Downloading corpus...\")\n",
    "shakespeare_text = get_book(url)\n",
    "tokens = tokenize(shakespeare_text)\n",
    "print(f\"Tokenization complete. Total tokens: {len(tokens)}\")\n",
    "\n",
    "# 2. Train Models\n",
    "print(\"Training Unigram Model...\")\n",
    "unigram_model = UnigramLM(tokens)\n",
    "\n",
    "print(\"Training Trigram (N=3) Model...\")\n",
    "trigram_model = NGramLM(N=3, tokens=tokens)\n",
    "\n",
    "# 3. Compare Output\n",
    "print(\"\\n--- Unigram Sample (Random Words) ---\")\n",
    "print(unigram_model.sample(20))\n",
    "\n",
    "print(\"\\n--- Trigram Sample (Context-Aware) ---\")\n",
    "print(trigram_model.sample(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
